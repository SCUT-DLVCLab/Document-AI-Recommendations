# Documenet AI Datasets

## Document Layout Analysis

|Dataset|Type|Description|Examples|Link|Permission|
|--|--|--|--|--|--|
|ICDAR 2013 Table Competition|Table|Datasets for the ICDAR 2013 Table Competition. Includes a total of **150** tables in PDF format: 75 tables in 27 excerpts from the EU and 75 tables in 40 excerpts from the US Government. An automatic text conversion using pdftk has also been included for convenience.|<img src="./img/dataset_img/ICDAR2013TableCompetition_1.jpg" width=350>|[ICDAR 2013 Table Competition](http://www.tamirhassan.com/html/competition.html)|-|
|Marmot-Dataset for table recognition|Table|**2000** pages in PDF format, the corresponding ground-truths were extracted utilizing the semi-automatic ground-truthing tool "Marmot". The dataset is composed of Chinese and English pages at the proportion of about 1:1.||1. [Marmot Dataset Homepage](https://www.icst.pku.edu.cn/cpdp/sjzy/index.htm) <br> 2. [Marmot Dataset for table recognition](https://www.icst.pku.edu.cn/cpdp/docs/20190424190300041510.zip)|Research‚úÖ|
|Marmot-Dataset for layout analysis of fixed layout documents|Documents|Contains **244** pages selected from 35 PDF documents. Primitive objects of page content include text, images and graphics.||1. [Marmot Dataset Homepage](https://www.icst.pku.edu.cn/cpdp/sjzy/index.htm) <br> 2. [Marmot Dataset for layout analysis of fixed layout documents](https://www.icst.pku.edu.cn/cpdp/docs/20190424193042344069.gz)|Research‚úÖ|
|PubTabNet|Table|Contains **568k+** images of tabular data annotated with the corresponding HTML representation of the tables|<img src="./img/dataset_img/PubTabNet_1.png" width=350>|[PubTabNet(GitHub)](https://github.com/ibm-aur-nlp/PubTabNet)|[LICENSE.md](https://github.com/ibm-aur-nlp/PubTabNet/blob/master/LICENSE.md)|
|PubLayNet|Document|**360k+** document images, of which the layout is annotated with both bounding boxes and polygonal segmentations|<img src="./img/dataset_img/PubLayNet_1.jpg" width=350>|[PubLayNet(GitHub)](https://github.com/ibm-aur-nlp/PubLayNet)||
|Document AI (MSRA) <br> TableBank|Table|Image-based table detection and recognition dataset, built with novel weak supervision from Word and Latex documents on the internet. Contains **417K** high-quality labeled tables||[TableBank(GitHub)](https://github.com/doc-analysis/TableBank)||
|Document AI (MSRA) <br> DocBank|Document|Large-scale dataset that is constructed using a weak supervision approach. Totally includes **500K** document pages, where 400K for training, 50K for validation and 50K for testing.||[DocBank(GitHub)](https://github.com/doc-analysis/DocBank)|Research‚úÖ <br> Commercial‚úÖ <br> Patent‚úÖ|
|IIIT-AR-13K|Business Documents|Business documents, more specifically annual reports. Contains a total of **13K** annotated page images with objects in five different popular categories ‚Äî table, figure, natural image, logo, and signature.|<img src="./img/dataset_img/IIIT-AR-13K_1.jpg" width=450>|[USODI IIIT-AR-13K Homepage](http://cvit.iiit.ac.in/usodi/iiitar13k.php) <br> [IIIT-AR-13K Dataset](http://cvit.iiit.ac.in/usodi/img/projects/detection/iiit-ar-13/dataset/IIIT-AR-13K_dataset.zip)||


## Visual Information Extraction

|Dataset|Type|Description|Examples|Link|Permission|
|--|--|--|--|--|--|
|FUNSD|Form|A dataset for Text Detection, Optical Character Recognition, Spatial Layout Analysis and Form Understanding. Contains **199** fully annotated forms, with 31485 words, 9707 semantic entities and 5304 relations|<img src="./img/dataset_img/FUNSD_1.jpg" width=350>|[FUNSD](https://guillaumejaume.github.io/FUNSD/)|Research‚úÖ|
|ICDAR2019-SROIE-Task3|Receipts|Receipt images with annotations and key information, 636 for training and 347 for testing.|<img src="./img/dataset_img/SROIE_1.jpg" width=350>|1. [ICDAR2019-SROIE Homepage](https://rrc.cvc.uab.es/?ch=13) <br> 2. [Unofficial, **train data only**, with preprocessing codes](https://github.com/zzzDavid/ICDAR-2019-SROIE) <br> 3. [üëçRecommended] [Unofficial, full set with train and test data](https://pan.baidu.com/s/17lTKLphTPHD5gi2frULeog) (extraction code: ecmo)|Share‚úÖ <br> Adapt‚úÖ|
|CORD|Receipts|Over **11,000** Indonesian receipts collected from shops and restaurants, with five superclass and 42 subclass labels|<img src="./img/dataset_img/CORD_1.png" width=350>|[CORD(GitHub)](https://github.com/clovaai/cord)|Share‚úÖ <br> Adapt‚úÖ|
|DeepForm|Receipts|**20,000** labeled receipts from political television ads bought in the 2012, 2014, and 2020 US elections.|-|1. [GitHub Home Page of the Paper](https://github.com/jstray/deepform) <br> 2. [DeepForm Dataset](https://drive.google.com/drive/folders/1bsV4A-8A9B7KZkzdbsBnCGKLMZftV2fQ?usp=sharing)|-|
|EATEN|Train ticket <br> Passport <br> Business card||<img src="./img/dataset_img/EATEN_1.jpg" width=350>|[EATEN(GitHub)](https://github.com/beacandler/EATEN)|-|
|EPHOIE|Examination Paper Head|Contains **1,494** images which are collected and scanned from real examination papers of various schools in China, crop the paper head regions which contains all key information. The texts are composed of handwritten and printed Chinese characters in horizontal and arbitrary quadrilateral shape. Complex layouts and noisy background also enhance the generalization of EPHOIE dataset|<img src="./img/dataset_img/EPHOIE_1.png">|[EPHOIE(GitHub)](https://github.com/HCIILAB/EPHOIE)|Research‚úÖ <br> Need application before use|
|Document AI (MSRA) <br> XFUND|Form|multilingual form understanding benchmark dataset that includes human-labeled forms with key-value pairs in 7 languages (Chinese, Japanese, Spanish, French, Italian, German, Portuguese)|-|[XFUND(GitHub)](https://github.com/doc-analysis/XFUND)|Share‚úÖ <br> Adapt‚úÖ|
